{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2532,
     "status": "error",
     "timestamp": 1587004807109,
     "user": {
      "displayName": "Zachary Hoffman",
      "photoUrl": "https://lh3.googleusercontent.com/-rKa4EKJ_fjo/AAAAAAAAAAI/AAAAAAAAFzQ/lt1t_MuKsbY/s64/photo.jpg",
      "userId": "08400015592921644849"
     },
     "user_tz": 240
    },
    "id": "4iEXYGEOxQpy",
    "outputId": "319c1ce7-2847-4530-bb7b-8a8d7af73d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm  # optional progress bar\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "from torch import Tensor\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "hyperparams = {\n",
    "    \"num_epochs\": 25,\n",
    "    \"batch_size\": 400,\n",
    "    \"learning_rate\": 1e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EX59duwmyEm3"
   },
   "outputs": [],
   "source": [
    "class HistoneDataset(Dataset):\n",
    "    def __init__(self, input_file, seq_file):\n",
    "        \"\"\"\n",
    "        :param input_file: the data file pathname\n",
    "        \"\"\"\n",
    "        self.id2seq = dict()\n",
    "        seqs = np.load(seq_file, allow_pickle=True)\n",
    "        for i in range(seqs.shape[0]):\n",
    "            row = seqs[i]\n",
    "            row[1].extend([0, 0])\n",
    "            self.id2seq[int(row[0])] = torch.tensor(row[1]).view(-1, 5).float()\n",
    "\n",
    "        # [50, 16000, 100, 7]\n",
    "        # [cell_types, genes, bins, (columns)]\n",
    "        # columns = GeneID, H3K27me3, H3K36me3, H3K4me1, H3K4me3, H3K9me3, Expression Value (same for entire bin)\n",
    "        # columns 0: GeneId, 1-5: Histone Marks, 6: Expression Value\n",
    "        npdata = np.load(input_file)\n",
    "        cell_types = npdata.files\n",
    "\n",
    "        # [cell_types, genes, bins, histomes]\n",
    "        input = []\n",
    "        # [cell_types, genes, expression]\n",
    "        output = []\n",
    "        # [cell_types, genes, expression]\n",
    "        ids = []\n",
    "        # types\n",
    "        types = []\n",
    "\n",
    "        for cell in cell_types:\n",
    "            cell_data = npdata[cell]\n",
    "            id = cell_data[:, 0, 0]\n",
    "            hm_data = cell_data[:, :, 1:6]\n",
    "            exp_values = cell_data[:, 0, 6]\n",
    "            ids.append(id)\n",
    "            input.append(hm_data)\n",
    "            output.append(exp_values)\n",
    "            types.extend([cell] * cell_data.shape[0])\n",
    "\n",
    "        # [cell_types*genes, bins, histomes]\n",
    "        input = np.concatenate(input, axis=0)\n",
    "        # [cell_types*genes, expression]\n",
    "        output = np.concatenate(output, axis=0)\n",
    "        ids = np.concatenate(ids, axis=0)\n",
    "\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.id = ids\n",
    "        self.type = np.asarray(types)\n",
    "\n",
    "        for x in input:\n",
    "            self.x.append(torch.tensor(x))\n",
    "\n",
    "        for y in output:\n",
    "            self.y.append(torch.tensor(y))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        len should return a the length of the dataset\n",
    "\n",
    "        :return: an integer length of the dataset\n",
    "        \"\"\"\n",
    "        # TODO: Override method to return length of dataset\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        getitem should return a tuple or dictionary of the data at some index\n",
    "        In this case, you should return your original and target sentence and\n",
    "        anything else you may need in training/validation/testing.\n",
    "\n",
    "        :param idx: the index for retrieval\n",
    "\n",
    "        :return: tuple or dictionary of the data\n",
    "        \"\"\"\n",
    "        # TODO: Override method to return the items in dataset\n",
    "        item = {\n",
    "            \"cell_type\": self.type[idx],\n",
    "            \"id\": self.id[idx],\n",
    "            \"x\": torch.cat((self.x[idx], self.id2seq[int(self.id[idx])]), dim=0),\n",
    "            \"y\": self.y[idx],\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hl369YPTyLkD"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def bn_function(self, inputs):\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input, Tensor):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "\n",
    "        bottleneck_output = self.bn_function(prev_features)\n",
    "\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        new_features = self.dropout(new_features)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class DenseBlock(nn.ModuleDict):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(Transition, self).__init__()\n",
    "        self.norm = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n",
    "        # prevent output from shrinking\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=(2, 1))\n",
    "\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0.5, num_classes=1, theta=0.5):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(1, num_init_features, kernel_size=7, stride=(2, 1),\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=(2, 1), padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = Transition(num_input_features=num_features,\n",
    "                                    num_output_features=int(num_features * theta))\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = int(num_features * theta)\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFWdvktAyPjk"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, validate_loader, location):\n",
    "    print(\"starting train\")\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "    optimizer = optim.Adam(model.parameters(), hyperparams['learning_rate'])\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    for epoch in range(hyperparams['num_epochs']):\n",
    "        model = model.train()\n",
    "        losses = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            x = batch['x']\n",
    "            y = batch['y']\n",
    "            x = x.unsqueeze(1)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = loss_fn(y_pred.squeeze(1), y)\n",
    "\n",
    "            loss.backward()  # calculate gradients\n",
    "            optimizer.step()  # update model weights\n",
    "\n",
    "            losses.insert(0, loss.item())\n",
    "            losses = losses[:100]\n",
    "        print(\"saving model\")\n",
    "        torch.save(model.state_dict(), location + 'model' + str(epoch) + '.pt')\n",
    "        print(epoch, \"epoch loss:\", np.mean(losses))\n",
    "        validate(model, validate_loader)\n",
    "\n",
    "\n",
    "def validate(model, validate_loader):\n",
    "    print(\"starting validation\")\n",
    "    loss_fn = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for batch in tqdm(validate_loader):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred.squeeze(1), y)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    print(\"mean loss:\", np.mean(losses))\n",
    "\n",
    "\n",
    "def test(model, test_loader, location):\n",
    "    print(\"starting test\")\n",
    "    model = model.eval()\n",
    "    classification = []\n",
    "\n",
    "    for batch in tqdm(test_loader):\n",
    "        x = batch['x']\n",
    "        cell_type = batch['cell_type']\n",
    "        id = batch['id']\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        for i in range(y_pred.size()[0]):\n",
    "            # print(cell_type[i].item(), id[i].item(), y_pred[i].item())\n",
    "            classification.append((cell_type[i].item() + \"_\" + str(int(id[i].item())), str(y_pred[i].item())))\n",
    "\n",
    "    df = pd.DataFrame(classification, columns=['id', 'expression'])\n",
    "    df.to_csv(location + 'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 196,
     "status": "error",
     "timestamp": 1587001374043,
     "user": {
      "displayName": "Zachary Hoffman",
      "photoUrl": "https://lh3.googleusercontent.com/-rKa4EKJ_fjo/AAAAAAAAAAI/AAAAAAAAFzQ/lt1t_MuKsbY/s64/photo.jpg",
      "userId": "08400015592921644849"
     },
     "user_tz": 240
    },
    "id": "VR2hUh_GykAR",
    "outputId": "32dd8d2d-d388-4ce2-b284-f37160262ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering test data cuda\n",
      "running training loop...\n",
      "starting train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:39<00:00, 11.32it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:06, 32.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "0 epoch loss: 3.3236157298088074\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 38.00it/s]\n",
      "  0%|                                                                                 | 1/1800 [00:00<05:13,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.3372390055656433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:40<00:00, 11.19it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:05, 37.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "1 epoch loss: 3.2935297012329103\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 39.04it/s]\n",
      "  0%|                                                                                         | 0/1800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.304830753803253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:39<00:00, 11.25it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:05, 38.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "2 epoch loss: 3.286192448139191\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 38.69it/s]\n",
      "  0%|                                                                                 | 1/1800 [00:00<04:54,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.3054303514957426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:38<00:00, 11.38it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:05, 35.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "3 epoch loss: 3.3425286197662354\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 39.09it/s]\n",
      "  0%|                                                                                 | 1/1800 [00:00<04:23,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.2940007722377778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:39<00:00, 11.31it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:05, 35.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "4 epoch loss: 3.2971162486076353\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 38.73it/s]\n",
      "  0%|                                                                                 | 1/1800 [00:00<04:50,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.287163416147232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:40<00:00, 11.23it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:05, 35.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "5 epoch loss: 3.249870662689209\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 38.71it/s]\n",
      "  0%|                                                                                 | 1/1800 [00:00<04:52,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.282512205839157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:38<00:00, 11.33it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:05, 37.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "6 epoch loss: 3.28410178899765\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 38.16it/s]\n",
      "  0%|                                                                                 | 1/1800 [00:00<04:48,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.2658091259002684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:39<00:00, 11.29it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:06, 32.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "7 epoch loss: 3.2826910424232483\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 38.20it/s]\n",
      "  0%|                                                                                 | 1/1800 [00:00<04:48,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.264050382375717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:40<00:00, 11.24it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:05, 33.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "8 epoch loss: 3.2231621241569517\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 38.43it/s]\n",
      "  0%|                                                                                 | 1/1800 [00:00<04:59,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.2536452066898347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1800/1800 [02:40<00:00, 11.18it/s]\n",
      "  2%|█▋                                                                                | 4/200 [00:00<00:06, 31.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n",
      "9 epoch loss: 3.2431330418586732\n",
      "starting validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 38.18it/s]\n",
      "  0%|                                                                                 | 1/1800 [00:00<04:27,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 3.247651263475418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████████▉  | 1751/1800 [02:34<00:04, 11.49it/s]"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "test_model = False\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "model = DenseNet(4, (2, 3, 3, 2), 16).to(device)\n",
    "location = './'\n",
    "test_file = location + 'data/eval.npz'\n",
    "train_file = location + 'data/train.npz'\n",
    "seq_file = location + 'data/encoded.npy'\n",
    "\n",
    "train_dataset = None\n",
    "validate_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "print(\"gathering train data\")\n",
    "train_loader = None\n",
    "if train_model:\n",
    "    dataset = HistoneDataset(train_file, seq_file)\n",
    "\n",
    "    split_amount = int(len(dataset) * 0.9)\n",
    "\n",
    "    train_dataset, validate_dataset = random_split(\n",
    "        dataset, (split_amount, len(dataset) - split_amount))\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=hyperparams['batch_size'], shuffle=True\n",
    "    )\n",
    "    validate_loader = DataLoader(\n",
    "        validate_dataset, batch_size=hyperparams['batch_size'], shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"gathering test data\", device)\n",
    "if test_model:\n",
    "    test_dataset = HistoneDataset(test_file, seq_file)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])\n",
    "\n",
    "if load_model:\n",
    "    print(\"loading saved model...\")\n",
    "    model.load_state_dict(torch.load(location + 'model.pt'))\n",
    "if train_model:\n",
    "    print(\"running training loop...\")\n",
    "    train(model, train_loader, validate_loader, location)\n",
    "    validate(model, validate_loader)\n",
    "if save_model:\n",
    "    print(\"saving model...\")\n",
    "    torch.save(model.state_dict(), location + 'model.pt')\n",
    "if test_model:\n",
    "    print(\"running testing loop...\")\n",
    "    test(model, test_loader, location)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNUsp9Xz2kd+zV+QeaLLJaw",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DLGenomicFinal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
