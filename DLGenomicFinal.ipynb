{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2532,
     "status": "error",
     "timestamp": 1587004807109,
     "user": {
      "displayName": "Zachary Hoffman",
      "photoUrl": "https://lh3.googleusercontent.com/-rKa4EKJ_fjo/AAAAAAAAAAI/AAAAAAAAFzQ/lt1t_MuKsbY/s64/photo.jpg",
      "userId": "08400015592921644849"
     },
     "user_tz": 240
    },
    "id": "4iEXYGEOxQpy",
    "outputId": "319c1ce7-2847-4530-bb7b-8a8d7af73d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm  # optional progress bar\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "from torch import Tensor\n",
    "import pprint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "hyperparams = {\n",
    "    \"num_epochs\": 1,\n",
    "    \"batch_size\": 100,\n",
    "    \"learning_rate\": 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EX59duwmyEm3"
   },
   "outputs": [],
   "source": [
    "def cells_to_id(cells):\n",
    "    cti = dict()\n",
    "    for i, cell in enumerate(cells):\n",
    "        cti[cell] = i\n",
    "    return cti\n",
    "\n",
    "train_cells = ['E065', 'E004', 'E066', 'E005', 'E012', 'E027', 'E053', 'E013', 'E028', 'E061', 'E109', 'E120', 'E062', 'E037', 'E038', 'E024', 'E105', 'E011', 'E106', 'E082', 'E097', 'E116', 'E098', 'E058',\n",
    "               'E117', 'E059', 'E070', 'E118', 'E085', 'E104', 'E119', 'E006', 'E127', 'E047', 'E094', 'E007', 'E054', 'E128', 'E095', 'E055', 'E114', 'E100', 'E056', 'E016', 'E122', 'E057', 'E123', 'E079', 'E003', 'E050']\n",
    "\n",
    "train_cells_dict = cells_to_id(train_cells)\n",
    "\n",
    "eval_cells = ['E065', 'E004', 'E066', 'E005', 'E012', 'E027', 'E053', 'E013', 'E028', 'E061', 'E109', 'E120', 'E062', 'E037', 'E038', 'E024', 'E071', 'E105', 'E087', 'E011', 'E106', 'E096', 'E082', 'E097',\n",
    "              'E116', 'E098', 'E058', 'E117', 'E084', 'E059', 'E070', 'E118', 'E085', 'E104', 'E119', 'E006', 'E112', 'E127', 'E047', 'E094', 'E007', 'E054', 'E113', 'E128', 'E095', 'E055', 'E114', 'E100', 'E056', 'E016', 'E122', 'E057', 'E123', 'E079', 'E003', 'E050']\n",
    "\n",
    "eval_cells_dict = cells_to_id(eval_cells)\n",
    "\n",
    "def onehot_encode(sequence):\n",
    "    molecule_to_vec = {\n",
    "        'A': 0,\n",
    "        'C': 1,\n",
    "        'T': 2,\n",
    "        'G': 3,\n",
    "        'N': 4\n",
    "    }\n",
    "    onehot_matrix = np.zeros((len(sequence), 5))\n",
    "    for i in range(len(sequence)):\n",
    "        onehot_matrix[i][molecule_to_vec[sequence[i]]] = 1\n",
    "\n",
    "    return onehot_matrix\n",
    "\n",
    "class HistoneDataset(Dataset):\n",
    "    def __init__(self, input_file, id2seq, cell=None):\n",
    "        \"\"\"\n",
    "        :param input_file: the data file pathname\n",
    "        \"\"\"\n",
    "        self.id2seq = id2seq\n",
    "\n",
    "        # [50, 16000, 100, 7]\n",
    "        # [cell_types, genes, bins, (columns)]\n",
    "        # columns = GeneID, H3K27me3, H3K36me3, H3K4me1, H3K4me3, H3K9me3, Expression Value (same for entire bin)\n",
    "        # columns 0: GeneId, 1-5: Histone Marks, 6: Expression Value\n",
    "        npdata = np.load(input_file)\n",
    "        cell_types = npdata.files\n",
    "\n",
    "        # [cell_types, genes, bins, histomes]\n",
    "        input = []\n",
    "        # [cell_types, genes, expression]\n",
    "        output = []\n",
    "        # [cell_types, genes, expression]\n",
    "        ids = []\n",
    "        # types\n",
    "        types = []\n",
    "\n",
    "        if cell is None:\n",
    "            for cell in cell_types:\n",
    "                cell_data = npdata[cell]\n",
    "                id = cell_data[:, 0, 0]\n",
    "                hm_data = cell_data[:, :, 1:6]\n",
    "                exp_values = cell_data[:, 0, 6]\n",
    "                ids.append(id)\n",
    "                input.append(hm_data)\n",
    "                output.append(exp_values)\n",
    "                type.extend([cell] * cell_data.shape[0])\n",
    "        else:\n",
    "            cell_data = npdata[cell]\n",
    "            id = cell_data[:, 0, 0]\n",
    "            hm_data = cell_data[:, :, 1:6]\n",
    "            exp_values = cell_data[:, 0, 6]\n",
    "            ids.append(id)\n",
    "            input.append(hm_data)\n",
    "            output.append(exp_values)\n",
    "            types.extend([cell] * cell_data.shape[0])\n",
    "\n",
    "        # [cell_types*genes, bins, histomes]\n",
    "        input = np.concatenate(input, axis=0)\n",
    "        # [cell_types*genes, expression]\n",
    "        output = np.concatenate(output, axis=0)\n",
    "        ids = np.concatenate(ids, axis=0)\n",
    "\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.id = ids\n",
    "        self.type = np.asarray(types)\n",
    "\n",
    "        for x in input:\n",
    "            self.x.append(torch.tensor(x))\n",
    "\n",
    "        for y in output:\n",
    "            self.y.append(torch.tensor(y))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        len should return a the length of the dataset\n",
    "\n",
    "        :return: an integer length of the dataset\n",
    "        \"\"\"\n",
    "        # TODO: Override method to return length of dataset\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        getitem should return a tuple or dictionary of the data at some index\n",
    "        In this case, you should return your original and target sentence and\n",
    "        anything else you may need in training/validation/testing.\n",
    "\n",
    "        :param idx: the index for retrieval\n",
    "\n",
    "        :return: tuple or dictionary of the data\n",
    "        \"\"\"\n",
    "        # TODO: Override method to return the items in dataset\n",
    "        item = {\n",
    "            \"cell_type\": self.type[idx],\n",
    "            \"id\": self.id[idx],\n",
    "            \"x\": torch.cat((self.x[idx], self.id2seq[self.id[idx]]), dim=0),\n",
    "            \"y\": self.y[idx],\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hl369YPTyLkD"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def bn_function(self, inputs):\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input, Tensor):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "\n",
    "        bottleneck_output = self.bn_function(prev_features)\n",
    "\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        new_features = self.dropout(new_features)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class DenseBlock(nn.ModuleDict):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(Transition, self).__init__()\n",
    "        self.norm = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n",
    "        # prevent output from shrinking\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=(2, 1))\n",
    "\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0.5, num_classes=1, theta=0.5):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(1, num_init_features, kernel_size=7, stride=(2, 1),\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=(2, 1), padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = Transition(num_input_features=num_features,\n",
    "                                    num_output_features=int(num_features * theta))\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = int(num_features * theta)\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def densenet():\n",
    "    # based on tochvision\n",
    "    model = DenseNet(4, (3, 3, 3, 3), 16)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFWdvktAyPjk"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, cell):\n",
    "    print(\"starting train\")\n",
    "    model = model.to(device)\n",
    "    loss_fn = torch.nn.MSELoss(\n",
    "        size_average=None, reduce=None, reduction='mean')\n",
    "    optimizer = optim.Adam(model.parameters(), hyperparams['learning_rate'])\n",
    "\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    for epoch in range(hyperparams['num_epochs']):\n",
    "        for batch in tqdm(train_loader):\n",
    "            x = batch['x']\n",
    "            y = batch['y']\n",
    "            x = x.unsqueeze(1)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = loss_fn(y_pred.squeeze(1), y)\n",
    "\n",
    "            loss.backward()  # calculate gradients\n",
    "            optimizer.step()  # update model weights\n",
    "\n",
    "            losses.insert(0, float(loss.cpu().item()))\n",
    "            losses = losses[:100]\n",
    "        print(\"avg loss:\", np.mean(losses))\n",
    "        torch.save(model.state_dict(), './model' + cell + '.pt')\n",
    "\n",
    "\n",
    "def validate(model, validate_loader):\n",
    "    print(\"starting validation\")\n",
    "    model = model.to(device)\n",
    "    loss_fn = torch.nn.MSELoss(\n",
    "        size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for batch in tqdm(validate_loader):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        # unsqueeze to create 1 input channel\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred.squeeze(1), y)\n",
    "\n",
    "        losses.append(float(loss.cpu().item()))\n",
    "\n",
    "    print(\"mean loss:\", np.mean(losses))\n",
    "\n",
    "\n",
    "def test(models, test_loaders, cell_types):\n",
    "    print(\"starting test\")\n",
    "    classification = []\n",
    "    for cell in cell_types:\n",
    "        test_loader = test_loaders[cell]\n",
    "        cell_models = []\n",
    "        if cell in train_cells:\n",
    "            model = models[train_cells_dict[cell]].to(device)\n",
    "            model = model.eval()\n",
    "            cell_models.append(model)\n",
    "        else:\n",
    "            for cell in train_cells:\n",
    "                model = models[train_cells_dict[cell]].to(device)\n",
    "                model = model.eval()\n",
    "                cell_models.append(model)\n",
    "\n",
    "        for batch in tqdm(test_loader):\n",
    "            x = batch['x']\n",
    "            id = batch['id']\n",
    "            cell_type = batch['cell_type']\n",
    "            x = x.unsqueeze(1)\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_preds = None\n",
    "            if len(cell_models) == 1:\n",
    "                y_preds = cell_models[0](x).unsqueeze(0)\n",
    "            else:\n",
    "                for model in cell_models:\n",
    "                    if y_preds is None:\n",
    "                        y_preds = model(x).unsqueeze(0)\n",
    "                    else:\n",
    "                        y_preds = torch.cat((y_preds, model(x).unsqueeze(0)))\n",
    "\n",
    "            y_pred = torch.mean(y_preds, 0)\n",
    "            # print(y_pred)\n",
    "\n",
    "            for i in range(y_pred.size()[0]):\n",
    "                # print(cell_type[i].item(), id[i].item(), y_pred[i].item())\n",
    "                classification.append(\n",
    "                    (str(cell_type[i].item()) + \"_\" + str(int(id[i].item())), str(y_pred[i].cpu().item())))\n",
    "\n",
    "    df = pd.DataFrame(classification, columns=['id', 'expression'])\n",
    "    df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 196,
     "status": "error",
     "timestamp": 1587001374043,
     "user": {
      "displayName": "Zachary Hoffman",
      "photoUrl": "https://lh3.googleusercontent.com/-rKa4EKJ_fjo/AAAAAAAAAAI/AAAAAAAAFzQ/lt1t_MuKsbY/s64/photo.jpg",
      "userId": "08400015592921644849"
     },
     "user_tz": 240
    },
    "id": "VR2hUh_GykAR",
    "outputId": "32dd8d2d-d388-4ce2-b284-f37160262ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n",
      "gathering train data cuda\n",
      "gathered sequences\n",
      "gathered train\n",
      "gathered test\n",
      "running training loop 0 out of 50 for E065\n",
      "starting train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████▏                                     | 81/152 [00:35<00:30,  2.35it/s]"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "test_model = False\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "print(\"Device\", device)\n",
    "\n",
    "models = dict()\n",
    "for cell in train_cells:\n",
    "    model = DenseNet(4, (3, 3, 3), 16)\n",
    "    models[train_cells_dict[cell]] = model\n",
    "    \n",
    "location = './'\n",
    "test_file = location + 'data/eval.npz'\n",
    "train_file = location + 'data/train.npz'\n",
    "seq_file = location + 'data/seq_data.csv'\n",
    "\n",
    "train_dataset = None\n",
    "validate_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "print(\"gathering train data\", device)\n",
    "train_loaders = dict()\n",
    "validate_loaders = dict()\n",
    "id2seq = dict()\n",
    "seqs = pd.read_csv(seq_file, index_col=False, names=[\"id\", \"sequence\"])\n",
    "for index, row in seqs.iterrows():\n",
    "    id2seq[row['id']] = torch.tensor(onehot_encode(row['sequence'])).float()\n",
    "print(\"gathered sequences\")\n",
    "\n",
    "if train_model:\n",
    "    train_datasets = dict()\n",
    "    validate_datasets = dict()\n",
    "    for cell in train_cells:\n",
    "        dataset = HistoneDataset(train_file, id2seq, cell)\n",
    "\n",
    "        split_amount = int(len(dataset) * 0.95)\n",
    "\n",
    "        train_dataset, validate_dataset = random_split(\n",
    "            dataset, (split_amount, len(dataset) - split_amount))\n",
    "\n",
    "        train_datasets[train_cells_dict[cell]] = train_dataset\n",
    "        validate_datasets[train_cells_dict[cell]] = validate_dataset\n",
    "    \n",
    "    for cell in train_cells:\n",
    "        train_loader = DataLoader(\n",
    "            train_datasets[train_cells_dict[cell]], batch_size=hyperparams['batch_size'], shuffle=True\n",
    "        )\n",
    "        validate_loader = DataLoader(\n",
    "            validate_datasets[train_cells_dict[cell]], batch_size=hyperparams['batch_size'], shuffle=True\n",
    "        )\n",
    "\n",
    "        train_loaders[train_cells_dict[cell]] = train_loader\n",
    "        validate_loaders[train_cells_dict[cell]] = validate_loader\n",
    "print(\"gathered train\")\n",
    "        \n",
    "cell_types = None\n",
    "test_loaders = dict()\n",
    "if test_model:\n",
    "    test_datasets = dict()\n",
    "    npzfile = np.load(test_file)\n",
    "    cell_types = npzfile.files\n",
    "    for cell in cell_types:\n",
    "        assert cell in eval_cells_dict\n",
    "        test_dataset = HistoneDataset(test_file, id2seq)\n",
    "        test_datasets[cell] = test_dataset\n",
    "    for cell in cell_types:\n",
    "        test_loader = DataLoader(test_datasets[cell], batch_size=hyperparams['batch_size'])\n",
    "        test_loaders[cell] = test_loader\n",
    "print(\"gathered test\")\n",
    "\n",
    "if load_model:\n",
    "    print(\"loading saved model...\")\n",
    "    for cell in train_cells:\n",
    "        models[train_cells_dict[cell]].load_state_dict(torch.load('./model' + cell + '.pt'))\n",
    "if train_model:\n",
    "    for i, cell in enumerate(train_cells):\n",
    "        print(\"running training loop\", i, \"out of\", len(train_cells), \"for\", cell)\n",
    "        model = models[train_cells_dict[cell]]\n",
    "        train(model, train_loaders[train_cells_dict[cell]], cell)\n",
    "        torch.cuda.empty_cache()\n",
    "        validate(model, validate_loaders[train_cells_dict[cell]])\n",
    "        torch.cuda.empty_cache()\n",
    "if save_model:\n",
    "    print(\"saving model...\")\n",
    "    for cell in train_cells:\n",
    "        torch.save(models[train_cells_dict[cell]].state_dict(), './model' + cell + '.pt')\n",
    "if test_model:\n",
    "    print(\"running testing loop...\")\n",
    "    test(models, test_loaders, cell_types)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-TeSHfPiEz6"
   },
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNUsp9Xz2kd+zV+QeaLLJaw",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DLGenomicFinal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
