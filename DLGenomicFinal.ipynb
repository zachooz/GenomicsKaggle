{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2532,
     "status": "error",
     "timestamp": 1587004807109,
     "user": {
      "displayName": "Zachary Hoffman",
      "photoUrl": "https://lh3.googleusercontent.com/-rKa4EKJ_fjo/AAAAAAAAAAI/AAAAAAAAFzQ/lt1t_MuKsbY/s64/photo.jpg",
      "userId": "08400015592921644849"
     },
     "user_tz": 240
    },
    "id": "4iEXYGEOxQpy",
    "outputId": "319c1ce7-2847-4530-bb7b-8a8d7af73d6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm  # optional progress bar\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "from torch import Tensor\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "hyperparams = {\n",
    "    \"num_epochs\": 8,\n",
    "    \"batch_size\": 200,\n",
    "    \"learning_rate\": 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EX59duwmyEm3"
   },
   "outputs": [],
   "source": [
    "class HistoneDataset(Dataset):\n",
    "    def __init__(self, input_file, seq_file):\n",
    "        \"\"\"\n",
    "        :param input_file: the data file pathname\n",
    "        \"\"\"\n",
    "        self.id2seq = dict()\n",
    "        seqs = np.load(seq_file, allow_pickle=True)\n",
    "        for i in range(seqs.shape[0]):\n",
    "            row = seqs[i]\n",
    "            self.id2seq[int(row[0])] = torch.tensor(row[1]).float()\n",
    "\n",
    "        # [50, 16000, 100, 7]\n",
    "        # [cell_types, genes, bins, (columns)]\n",
    "        # columns = GeneID, H3K27me3, H3K36me3, H3K4me1, H3K4me3, H3K9me3, Expression Value (same for entire bin)\n",
    "        # columns 0: GeneId, 1-5: Histone Marks, 6: Expression Value\n",
    "        npdata = np.load(input_file)\n",
    "        cell_types = npdata.files\n",
    "\n",
    "        # [cell_types, genes, bins, histomes]\n",
    "        input = []\n",
    "        # [cell_types, genes, expression]\n",
    "        output = []\n",
    "        # [cell_types, genes, expression]\n",
    "        ids = []\n",
    "        # types\n",
    "        types = []\n",
    "\n",
    "        for cell in cell_types:\n",
    "            cell_data = npdata[cell]\n",
    "            id = cell_data[:, 0, 0]\n",
    "            hm_data = cell_data[:, :, 1:6]\n",
    "            exp_values = cell_data[:, 0, 6]\n",
    "            ids.append(id)\n",
    "            input.append(hm_data)\n",
    "            output.append(exp_values)\n",
    "            types.extend([cell] * cell_data.shape[0])\n",
    "\n",
    "        # [cell_types*genes, bins, histomes]\n",
    "        input = np.concatenate(input, axis=0)\n",
    "        # [cell_types*genes, expression]\n",
    "        output = np.concatenate(output, axis=0)\n",
    "        ids = np.concatenate(ids, axis=0)\n",
    "\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.id = ids\n",
    "        self.type = np.asarray(types)\n",
    "\n",
    "        for x in input:\n",
    "            self.x.append(torch.tensor(x))\n",
    "\n",
    "        for y in output:\n",
    "            self.y.append(torch.tensor(y))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        len should return a the length of the dataset\n",
    "\n",
    "        :return: an integer length of the dataset\n",
    "        \"\"\"\n",
    "        # TODO: Override method to return length of dataset\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        getitem should return a tuple or dictionary of the data at some index\n",
    "        In this case, you should return your original and target sentence and\n",
    "        anything else you may need in training/validation/testing.\n",
    "\n",
    "        :param idx: the index for retrieval\n",
    "\n",
    "        :return: tuple or dictionary of the data\n",
    "        \"\"\"\n",
    "        # TODO: Override method to return the items in dataset\n",
    "        item = {\n",
    "            \"cell_type\": self.type[idx],\n",
    "            \"id\": self.id[idx],\n",
    "            \"x\": torch.cat((self.x[idx], self.id2seq[int(self.id[idx])]), dim=0),\n",
    "            \"y\": self.y[idx],\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hl369YPTyLkD"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def bn_function(self, inputs):\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    def forward(self, input):\n",
    "        if isinstance(input, Tensor):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "\n",
    "        bottleneck_output = self.bn_function(prev_features)\n",
    "\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        new_features = self.dropout(new_features)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class DenseBlock(nn.ModuleDict):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(Transition, self).__init__()\n",
    "        self.norm = nn.BatchNorm2d(num_input_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n",
    "        # prevent output from shrinking\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=(2, 1))\n",
    "\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0.5, num_classes=1, theta=0.5):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(1, num_init_features, kernel_size=7, stride=(2, 1),\n",
    "                                padding=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=(2, 1), padding=1)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = Transition(num_input_features=num_features,\n",
    "                                    num_output_features=int(num_features * theta))\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = int(num_features * theta)\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFWdvktAyPjk"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, validate_loader, location):\n",
    "    print(\"starting train\")\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "    optimizer = optim.Adam(model.parameters(), hyperparams['learning_rate'])\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    for epoch in range(hyperparams['num_epochs']):\n",
    "        model = model.train()\n",
    "        losses = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            x = batch['x']\n",
    "            y = batch['y']\n",
    "            x = x.unsqueeze(1)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = loss_fn(y_pred.squeeze(1), y)\n",
    "\n",
    "            loss.backward()  # calculate gradients\n",
    "            optimizer.step()  # update model weights\n",
    "\n",
    "            losses.insert(0, loss.item())\n",
    "            losses = losses[:100]\n",
    "        print(\"saving model\")\n",
    "        torch.save(model.state_dict(), location + 'model' + str(epoch) + '.pt')\n",
    "        print(epoch, \"epoch loss:\", np.mean(losses))\n",
    "        validate(model, validate_loader)\n",
    "\n",
    "\n",
    "def validate(model, validate_loader):\n",
    "    print(\"starting validation\")\n",
    "    loss_fn = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for batch in tqdm(validate_loader):\n",
    "        x = batch['x']\n",
    "        y = batch['y']\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred.squeeze(1), y)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    print(\"mean loss:\", np.mean(losses))\n",
    "\n",
    "\n",
    "def test(model, test_loader, location):\n",
    "    print(\"starting test\")\n",
    "    model = model.eval()\n",
    "    classification = []\n",
    "\n",
    "    for batch in tqdm(test_loader):\n",
    "        x = batch['x']\n",
    "        cell_type = batch['cell_type']\n",
    "        id = batch['id']\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        for i in range(y_pred.size()[0]):\n",
    "            # print(cell_type[i].item(), id[i].item(), y_pred[i].item())\n",
    "            classification.append((cell_type[i].item() + \"_\" + str(int(id[i].item())), str(y_pred[i].item())))\n",
    "\n",
    "    df = pd.DataFrame(classification, columns=['id', 'expression'])\n",
    "    df.to_csv(location + 'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 196,
     "status": "error",
     "timestamp": 1587001374043,
     "user": {
      "displayName": "Zachary Hoffman",
      "photoUrl": "https://lh3.googleusercontent.com/-rKa4EKJ_fjo/AAAAAAAAAAI/AAAAAAAAFzQ/lt1t_MuKsbY/s64/photo.jpg",
      "userId": "08400015592921644849"
     },
     "user_tz": 240
    },
    "id": "VR2hUh_GykAR",
    "outputId": "32dd8d2d-d388-4ce2-b284-f37160262ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/3600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering test data cuda\n",
      "running training loop...\n",
      "starting train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Tensors must have same number of dimensions: got 2 and 1 at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\TH/generic/THTensor.cpp:603",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-776a2295562f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"running training loop...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-5deb89be7a9a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, validate_loader, location)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zacha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1127\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1128\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zacha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zacha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zacha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zacha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zacha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-072723c77e88>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;34m\"cell_type\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;34m\"id\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[1;34m\"x\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;34m\"y\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         }\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 2 and 1 at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\TH/generic/THTensor.cpp:603"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "test_model = False\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "model = DenseNet(4, (2, 2, 2), 16).to(device)\n",
    "location = './'\n",
    "test_file = location + 'data/eval.npz'\n",
    "train_file = location + 'data/train.npz'\n",
    "seq_file = location + 'data/encoded.npy'\n",
    "\n",
    "train_dataset = None\n",
    "validate_dataset = None\n",
    "test_dataset = None\n",
    "\n",
    "print(\"gathering train data\")\n",
    "train_loader = None\n",
    "if train_model:\n",
    "    dataset = HistoneDataset(train_file, seq_file)\n",
    "\n",
    "    split_amount = int(len(dataset) * 0.9)\n",
    "\n",
    "    train_dataset, validate_dataset = random_split(\n",
    "        dataset, (split_amount, len(dataset) - split_amount))\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=hyperparams['batch_size'], shuffle=True\n",
    "    )\n",
    "    validate_loader = DataLoader(\n",
    "        validate_dataset, batch_size=hyperparams['batch_size'], shuffle=True\n",
    "    )\n",
    "\n",
    "print(\"gathering test data\", device)\n",
    "if test_model:\n",
    "    test_dataset = HistoneDataset(test_file, seq_file)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])\n",
    "\n",
    "if load_model:\n",
    "    print(\"loading saved model...\")\n",
    "    model.load_state_dict(torch.load(location + 'model.pt'))\n",
    "if train_model:\n",
    "    print(\"running training loop...\")\n",
    "    train(model, train_loader, validate_loader, location)\n",
    "    validate(model, validate_loader)\n",
    "if save_model:\n",
    "    print(\"saving model...\")\n",
    "    torch.save(model.state_dict(), location + 'model.pt')\n",
    "if test_model:\n",
    "    print(\"running testing loop...\")\n",
    "    test(model, test_loader, location)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-TeSHfPiEz6"
   },
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNUsp9Xz2kd+zV+QeaLLJaw",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DLGenomicFinal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
